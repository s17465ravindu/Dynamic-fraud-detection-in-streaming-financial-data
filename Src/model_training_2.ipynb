{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.combine import SMOTETomek,SMOTEENN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.multiprocessing import set_start_method\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\Uni Docs\\DSC4996\\Dynamic_fraud_detection_system\\Data\\pre_processed_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_trans = df[df['Class'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>406.0</td>\n",
       "      <td>-2.312227</td>\n",
       "      <td>1.951992</td>\n",
       "      <td>-1.609851</td>\n",
       "      <td>3.997906</td>\n",
       "      <td>-0.522188</td>\n",
       "      <td>-1.426545</td>\n",
       "      <td>-2.537387</td>\n",
       "      <td>1.391657</td>\n",
       "      <td>-2.770089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517232</td>\n",
       "      <td>-0.035049</td>\n",
       "      <td>-0.465211</td>\n",
       "      <td>0.320198</td>\n",
       "      <td>0.044519</td>\n",
       "      <td>0.177840</td>\n",
       "      <td>0.261145</td>\n",
       "      <td>-0.143276</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>472.0</td>\n",
       "      <td>-3.043541</td>\n",
       "      <td>-3.157307</td>\n",
       "      <td>1.088463</td>\n",
       "      <td>2.288644</td>\n",
       "      <td>1.359805</td>\n",
       "      <td>-1.064823</td>\n",
       "      <td>0.325574</td>\n",
       "      <td>-0.067794</td>\n",
       "      <td>-0.270953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.661696</td>\n",
       "      <td>0.435477</td>\n",
       "      <td>1.375966</td>\n",
       "      <td>-0.293803</td>\n",
       "      <td>0.279798</td>\n",
       "      <td>-0.145362</td>\n",
       "      <td>-0.252773</td>\n",
       "      <td>0.035764</td>\n",
       "      <td>529.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4886</th>\n",
       "      <td>4462.0</td>\n",
       "      <td>-2.303350</td>\n",
       "      <td>1.759247</td>\n",
       "      <td>-0.359745</td>\n",
       "      <td>2.330243</td>\n",
       "      <td>-0.821628</td>\n",
       "      <td>-0.075788</td>\n",
       "      <td>0.562320</td>\n",
       "      <td>-0.399147</td>\n",
       "      <td>-0.238253</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294166</td>\n",
       "      <td>-0.932391</td>\n",
       "      <td>0.172726</td>\n",
       "      <td>-0.087330</td>\n",
       "      <td>-0.156114</td>\n",
       "      <td>-0.542628</td>\n",
       "      <td>0.039566</td>\n",
       "      <td>-0.153029</td>\n",
       "      <td>239.93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6072</th>\n",
       "      <td>6986.0</td>\n",
       "      <td>-4.397974</td>\n",
       "      <td>1.358367</td>\n",
       "      <td>-2.592844</td>\n",
       "      <td>2.679787</td>\n",
       "      <td>-1.128131</td>\n",
       "      <td>-1.706536</td>\n",
       "      <td>-3.496197</td>\n",
       "      <td>-0.248778</td>\n",
       "      <td>-0.247768</td>\n",
       "      <td>...</td>\n",
       "      <td>0.573574</td>\n",
       "      <td>0.176968</td>\n",
       "      <td>-0.436207</td>\n",
       "      <td>-0.053502</td>\n",
       "      <td>0.252405</td>\n",
       "      <td>-0.657488</td>\n",
       "      <td>-0.827136</td>\n",
       "      <td>0.849573</td>\n",
       "      <td>59.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6293</th>\n",
       "      <td>7519.0</td>\n",
       "      <td>1.234235</td>\n",
       "      <td>3.019740</td>\n",
       "      <td>-4.304597</td>\n",
       "      <td>4.732795</td>\n",
       "      <td>3.624201</td>\n",
       "      <td>-1.357746</td>\n",
       "      <td>1.713445</td>\n",
       "      <td>-0.496358</td>\n",
       "      <td>-1.282858</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.379068</td>\n",
       "      <td>-0.704181</td>\n",
       "      <td>-0.656805</td>\n",
       "      <td>-1.632653</td>\n",
       "      <td>1.488901</td>\n",
       "      <td>0.566797</td>\n",
       "      <td>-0.010016</td>\n",
       "      <td>0.146793</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278813</th>\n",
       "      <td>169142.0</td>\n",
       "      <td>-1.927883</td>\n",
       "      <td>1.125653</td>\n",
       "      <td>-4.518331</td>\n",
       "      <td>1.749293</td>\n",
       "      <td>-1.566487</td>\n",
       "      <td>-2.010494</td>\n",
       "      <td>-0.882850</td>\n",
       "      <td>0.697211</td>\n",
       "      <td>-2.064945</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778584</td>\n",
       "      <td>-0.319189</td>\n",
       "      <td>0.639419</td>\n",
       "      <td>-0.294885</td>\n",
       "      <td>0.537503</td>\n",
       "      <td>0.788395</td>\n",
       "      <td>0.292680</td>\n",
       "      <td>0.147968</td>\n",
       "      <td>390.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279090</th>\n",
       "      <td>169347.0</td>\n",
       "      <td>1.378559</td>\n",
       "      <td>1.289381</td>\n",
       "      <td>-5.004247</td>\n",
       "      <td>1.411850</td>\n",
       "      <td>0.442581</td>\n",
       "      <td>-1.326536</td>\n",
       "      <td>-1.413170</td>\n",
       "      <td>0.248525</td>\n",
       "      <td>-1.127396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.370612</td>\n",
       "      <td>0.028234</td>\n",
       "      <td>-0.145640</td>\n",
       "      <td>-0.081049</td>\n",
       "      <td>0.521875</td>\n",
       "      <td>0.739467</td>\n",
       "      <td>0.389152</td>\n",
       "      <td>0.186637</td>\n",
       "      <td>0.76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279096</th>\n",
       "      <td>169351.0</td>\n",
       "      <td>-0.676143</td>\n",
       "      <td>1.126366</td>\n",
       "      <td>-2.213700</td>\n",
       "      <td>0.468308</td>\n",
       "      <td>-1.120541</td>\n",
       "      <td>-0.003346</td>\n",
       "      <td>-2.234739</td>\n",
       "      <td>1.210158</td>\n",
       "      <td>-0.652250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.751826</td>\n",
       "      <td>0.834108</td>\n",
       "      <td>0.190944</td>\n",
       "      <td>0.032070</td>\n",
       "      <td>-0.739695</td>\n",
       "      <td>0.471111</td>\n",
       "      <td>0.385107</td>\n",
       "      <td>0.194361</td>\n",
       "      <td>77.89</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280081</th>\n",
       "      <td>169966.0</td>\n",
       "      <td>-3.113832</td>\n",
       "      <td>0.585864</td>\n",
       "      <td>-5.399730</td>\n",
       "      <td>1.817092</td>\n",
       "      <td>-0.840618</td>\n",
       "      <td>-2.943548</td>\n",
       "      <td>-2.208002</td>\n",
       "      <td>1.058733</td>\n",
       "      <td>-1.632333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.583276</td>\n",
       "      <td>-0.269209</td>\n",
       "      <td>-0.456108</td>\n",
       "      <td>-0.183659</td>\n",
       "      <td>-0.328168</td>\n",
       "      <td>0.606116</td>\n",
       "      <td>0.884876</td>\n",
       "      <td>-0.253700</td>\n",
       "      <td>245.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280611</th>\n",
       "      <td>170348.0</td>\n",
       "      <td>1.991976</td>\n",
       "      <td>0.158476</td>\n",
       "      <td>-2.583441</td>\n",
       "      <td>0.408670</td>\n",
       "      <td>1.151147</td>\n",
       "      <td>-0.096695</td>\n",
       "      <td>0.223050</td>\n",
       "      <td>-0.068384</td>\n",
       "      <td>0.577829</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164350</td>\n",
       "      <td>-0.295135</td>\n",
       "      <td>-0.072173</td>\n",
       "      <td>-0.450261</td>\n",
       "      <td>0.313267</td>\n",
       "      <td>-0.289617</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>-0.015309</td>\n",
       "      <td>42.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>473 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "534        406.0 -2.312227  1.951992 -1.609851  3.997906 -0.522188 -1.426545   \n",
       "616        472.0 -3.043541 -3.157307  1.088463  2.288644  1.359805 -1.064823   \n",
       "4886      4462.0 -2.303350  1.759247 -0.359745  2.330243 -0.821628 -0.075788   \n",
       "6072      6986.0 -4.397974  1.358367 -2.592844  2.679787 -1.128131 -1.706536   \n",
       "6293      7519.0  1.234235  3.019740 -4.304597  4.732795  3.624201 -1.357746   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "278813  169142.0 -1.927883  1.125653 -4.518331  1.749293 -1.566487 -2.010494   \n",
       "279090  169347.0  1.378559  1.289381 -5.004247  1.411850  0.442581 -1.326536   \n",
       "279096  169351.0 -0.676143  1.126366 -2.213700  0.468308 -1.120541 -0.003346   \n",
       "280081  169966.0 -3.113832  0.585864 -5.399730  1.817092 -0.840618 -2.943548   \n",
       "280611  170348.0  1.991976  0.158476 -2.583441  0.408670  1.151147 -0.096695   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "534    -2.537387  1.391657 -2.770089  ...  0.517232 -0.035049 -0.465211   \n",
       "616     0.325574 -0.067794 -0.270953  ...  0.661696  0.435477  1.375966   \n",
       "4886    0.562320 -0.399147 -0.238253  ... -0.294166 -0.932391  0.172726   \n",
       "6072   -3.496197 -0.248778 -0.247768  ...  0.573574  0.176968 -0.436207   \n",
       "6293    1.713445 -0.496358 -1.282858  ... -0.379068 -0.704181 -0.656805   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "278813 -0.882850  0.697211 -2.064945  ...  0.778584 -0.319189  0.639419   \n",
       "279090 -1.413170  0.248525 -1.127396  ...  0.370612  0.028234 -0.145640   \n",
       "279096 -2.234739  1.210158 -0.652250  ...  0.751826  0.834108  0.190944   \n",
       "280081 -2.208002  1.058733 -1.632333  ...  0.583276 -0.269209 -0.456108   \n",
       "280611  0.223050 -0.068384  0.577829  ... -0.164350 -0.295135 -0.072173   \n",
       "\n",
       "             V24       V25       V26       V27       V28  Amount  Class  \n",
       "534     0.320198  0.044519  0.177840  0.261145 -0.143276    0.00      1  \n",
       "616    -0.293803  0.279798 -0.145362 -0.252773  0.035764  529.00      1  \n",
       "4886   -0.087330 -0.156114 -0.542628  0.039566 -0.153029  239.93      1  \n",
       "6072   -0.053502  0.252405 -0.657488 -0.827136  0.849573   59.00      1  \n",
       "6293   -1.632653  1.488901  0.566797 -0.010016  0.146793    1.00      1  \n",
       "...          ...       ...       ...       ...       ...     ...    ...  \n",
       "278813 -0.294885  0.537503  0.788395  0.292680  0.147968  390.00      1  \n",
       "279090 -0.081049  0.521875  0.739467  0.389152  0.186637    0.76      1  \n",
       "279096  0.032070 -0.739695  0.471111  0.385107  0.194361   77.89      1  \n",
       "280081 -0.183659 -0.328168  0.606116  0.884876 -0.253700  245.00      1  \n",
       "280611 -0.450261  0.313267 -0.289617  0.002988 -0.015309   42.53      1  \n",
       "\n",
       "[473 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "genuine_trans = df[df['Class'] == 0].sample(len(fraud_trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced = pd.concat([fraud_trans,genuine_trans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df_balanced.drop('Class',axis=1)\n",
    "target = df_balanced['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "features[['Time','Amount']] = scaler.fit_transform(features[['Time','Amount']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training - Data Under Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgt = LogisticRegression()\n",
    "lgt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lgt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9157894736842105\n",
      "Precision: 0.9545454545454546\n",
      "Recall: 0.875\n",
      "F1-score: 0.9130434782608695\n",
      "MCC: 0.8346696343861983\n"
     ]
    }
   ],
   "source": [
    "accuracy_lgt = accuracy_score(y_test, y_pred)\n",
    "precision_lgt = precision_score(y_test, y_pred)\n",
    "recall_lgt = recall_score(y_test, y_pred)\n",
    "f1_score_lgt = f1_score(y_test, y_pred)\n",
    "mcc_lgt = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_lgt)\n",
    "print(\"Precision:\", precision_lgt)\n",
    "print(\"Recall:\", recall_lgt)\n",
    "print(\"F1-score:\", f1_score_lgt)\n",
    "print(\"MCC:\", mcc_lgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9210526315789473\n",
      "Precision: 0.9764705882352941\n",
      "Recall: 0.8645833333333334\n",
      "F1-score: 0.9171270718232044\n",
      "MCC: 0.8479710021632604\n"
     ]
    }
   ],
   "source": [
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "precision_svm = precision_score(y_test, y_pred_svm)\n",
    "recall_svm = recall_score(y_test, y_pred_svm)\n",
    "f1_score_svm = f1_score(y_test, y_pred_svm)\n",
    "mcc_svm = matthews_corrcoef(y_test, y_pred_svm)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_svm)\n",
    "print(\"Precision:\", precision_svm)\n",
    "print(\"Recall:\", recall_svm)\n",
    "print(\"F1-score:\", f1_score_svm)\n",
    "print(\"MCC:\", mcc_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE GAN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 123\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "BATCH_SIZE=1000\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "NUM_WORKERS=0\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_trans = fraud_trans.drop('Class',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_trans[['Time','Amount']] = scaler.fit_transform(fraud_trans[['Time','Amount']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreditCardData(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = torch.tensor(self.data.iloc[index]).float()\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data: pd.DataFrame, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Empty prepare_data method left in intentionally. \n",
    "        https://pytorch-lightning.readthedocs.io/en/latest/data/datamodule.html#prepare-data\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        train_df, test_df = train_test_split(self.data, random_state=123, test_size=0.2)\n",
    "        data_mean = train_df.mean()\n",
    "        data_std = train_df.std()\n",
    "        train_norm = (train_df - data_mean)/data_std\n",
    "        test_norm = (test_df - data_mean)/data_std\n",
    "        self.train_df = train_norm\n",
    "        self.test_df = test_norm\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset=CreditCardData(self.train_df), batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "    \n",
    "    def valid_dataloader(self):\n",
    "        return DataLoader(CreditCardData(self.val_df), batch_size=self.batch_size, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(CreditCardData(self.test_df), batch_size=self.batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(30, 150),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(150, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tensor = torch.sigmoid(self.sequential(x))\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super().__init__()\n",
    "        self.sequential = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 100),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(100, 80),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(80, 40),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(40, 30)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.sequential(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(pl.LightningModule):\n",
    "    def __init__(self, latent_dim=100, lr=0.002):\n",
    "        super().__init__()\n",
    "        self.automatic_optimization = False\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.generator = Generator(latent_dim = self.hparams.latent_dim)\n",
    "        self.discriminator = Discriminator()\n",
    "        \n",
    "        self.validation_z = torch.randn(6, self.hparams.latent_dim)\n",
    "        \n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "    \n",
    "    \n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return F.binary_cross_entropy(y_hat, y)\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch ): \n",
    "        real_data = batch\n",
    "        \n",
    "        opt_1, opt_2 = self.optimizers()\n",
    "        z = torch.randn(real_data.size(0), self.hparams.latent_dim)\n",
    "\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr )\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr )\n",
    "        return [opt_g, opt_d], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = GAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: d:\\Uni Docs\\DSC4996\\Dynamic_fraud_detection_system\\Src\\lightning_logs\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | generator     | Generator     | 22.7 K\n",
      "1 | discriminator | Discriminator | 13.5 K\n",
      "------------------------------------------------\n",
      "36.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "36.2 K    Total params\n",
      "0.145     Total estimated model params size (MB)\n",
      "c:\\Users\\Ravin\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\Ravin\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:281: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d5abe96a0843a1bf1448171d0dc97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    }
   ],
   "source": [
    "dm = DataModule(fraud_trans)\n",
    "trainer = pl.Trainer(max_epochs=100)\n",
    "trainer.fit(gnn, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1255, -0.0655,  0.1160,  ..., -0.0322, -0.1999, -0.1380],\n",
       "        [-0.1705, -0.0180,  0.0969,  ..., -0.0784, -0.1806, -0.0667],\n",
       "        [-0.1942, -0.0403,  0.0915,  ..., -0.0915, -0.2061, -0.1551],\n",
       "        ...,\n",
       "        [-0.1414, -0.0012,  0.1088,  ..., -0.0505, -0.2204, -0.0626],\n",
       "        [-0.1425,  0.0363,  0.1381,  ..., -0.0976, -0.2072, -0.1303],\n",
       "        [-0.1539, -0.0324,  0.1680,  ..., -0.0687, -0.1842, -0.0673]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.randn(175000, 100)\n",
    "output = gnn(z)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_fraud_df =  pd.DataFrame(output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_fraud_df['Class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.125472</td>\n",
       "      <td>-0.065518</td>\n",
       "      <td>0.116015</td>\n",
       "      <td>-0.078386</td>\n",
       "      <td>-0.070315</td>\n",
       "      <td>-0.122219</td>\n",
       "      <td>0.017378</td>\n",
       "      <td>-0.126982</td>\n",
       "      <td>0.037647</td>\n",
       "      <td>-0.064911</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.093300</td>\n",
       "      <td>-0.030247</td>\n",
       "      <td>0.125862</td>\n",
       "      <td>-0.075998</td>\n",
       "      <td>-0.203211</td>\n",
       "      <td>-0.113421</td>\n",
       "      <td>-0.032202</td>\n",
       "      <td>-0.199925</td>\n",
       "      <td>-0.137983</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.170451</td>\n",
       "      <td>-0.018026</td>\n",
       "      <td>0.096852</td>\n",
       "      <td>-0.065626</td>\n",
       "      <td>0.072934</td>\n",
       "      <td>-0.109999</td>\n",
       "      <td>0.105688</td>\n",
       "      <td>-0.139326</td>\n",
       "      <td>-0.024676</td>\n",
       "      <td>-0.038572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>-0.029055</td>\n",
       "      <td>0.068315</td>\n",
       "      <td>-0.035170</td>\n",
       "      <td>-0.226974</td>\n",
       "      <td>-0.050105</td>\n",
       "      <td>-0.078412</td>\n",
       "      <td>-0.180631</td>\n",
       "      <td>-0.066747</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.194220</td>\n",
       "      <td>-0.040289</td>\n",
       "      <td>0.091481</td>\n",
       "      <td>-0.017461</td>\n",
       "      <td>-0.019851</td>\n",
       "      <td>-0.126800</td>\n",
       "      <td>0.100452</td>\n",
       "      <td>-0.153539</td>\n",
       "      <td>-0.021744</td>\n",
       "      <td>-0.032416</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.144041</td>\n",
       "      <td>0.039883</td>\n",
       "      <td>0.146865</td>\n",
       "      <td>-0.086697</td>\n",
       "      <td>-0.240962</td>\n",
       "      <td>-0.144000</td>\n",
       "      <td>-0.091510</td>\n",
       "      <td>-0.206072</td>\n",
       "      <td>-0.155127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.093280</td>\n",
       "      <td>-0.046098</td>\n",
       "      <td>0.146738</td>\n",
       "      <td>-0.053210</td>\n",
       "      <td>0.109236</td>\n",
       "      <td>-0.054069</td>\n",
       "      <td>0.056616</td>\n",
       "      <td>-0.196522</td>\n",
       "      <td>-0.041214</td>\n",
       "      <td>-0.055622</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.050068</td>\n",
       "      <td>0.019684</td>\n",
       "      <td>0.018286</td>\n",
       "      <td>0.022157</td>\n",
       "      <td>-0.161324</td>\n",
       "      <td>-0.015622</td>\n",
       "      <td>-0.043951</td>\n",
       "      <td>-0.208535</td>\n",
       "      <td>-0.073233</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.150356</td>\n",
       "      <td>-0.012215</td>\n",
       "      <td>0.128281</td>\n",
       "      <td>-0.067569</td>\n",
       "      <td>0.016128</td>\n",
       "      <td>-0.088675</td>\n",
       "      <td>0.093983</td>\n",
       "      <td>-0.178635</td>\n",
       "      <td>0.011020</td>\n",
       "      <td>-0.070727</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137437</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.095065</td>\n",
       "      <td>-0.012209</td>\n",
       "      <td>-0.176238</td>\n",
       "      <td>-0.080777</td>\n",
       "      <td>-0.105282</td>\n",
       "      <td>-0.187900</td>\n",
       "      <td>-0.072092</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174995</th>\n",
       "      <td>-0.146697</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.108352</td>\n",
       "      <td>-0.014537</td>\n",
       "      <td>0.086563</td>\n",
       "      <td>-0.068104</td>\n",
       "      <td>0.024547</td>\n",
       "      <td>-0.137124</td>\n",
       "      <td>-0.050225</td>\n",
       "      <td>-0.040411</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033816</td>\n",
       "      <td>0.013552</td>\n",
       "      <td>0.039731</td>\n",
       "      <td>-0.039308</td>\n",
       "      <td>-0.159591</td>\n",
       "      <td>-0.068564</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>-0.207346</td>\n",
       "      <td>-0.057379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174996</th>\n",
       "      <td>-0.106352</td>\n",
       "      <td>0.031905</td>\n",
       "      <td>0.157137</td>\n",
       "      <td>-0.107703</td>\n",
       "      <td>0.037686</td>\n",
       "      <td>-0.094337</td>\n",
       "      <td>0.061924</td>\n",
       "      <td>-0.076955</td>\n",
       "      <td>-0.095596</td>\n",
       "      <td>-0.074425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058858</td>\n",
       "      <td>-0.046635</td>\n",
       "      <td>0.119674</td>\n",
       "      <td>-0.080931</td>\n",
       "      <td>-0.211604</td>\n",
       "      <td>-0.091140</td>\n",
       "      <td>-0.044451</td>\n",
       "      <td>-0.162417</td>\n",
       "      <td>-0.099701</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174997</th>\n",
       "      <td>-0.141414</td>\n",
       "      <td>-0.001164</td>\n",
       "      <td>0.108775</td>\n",
       "      <td>-0.037603</td>\n",
       "      <td>0.031697</td>\n",
       "      <td>-0.133319</td>\n",
       "      <td>0.009418</td>\n",
       "      <td>-0.163063</td>\n",
       "      <td>-0.022776</td>\n",
       "      <td>-0.055269</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067018</td>\n",
       "      <td>0.008169</td>\n",
       "      <td>0.094791</td>\n",
       "      <td>-0.040277</td>\n",
       "      <td>-0.154987</td>\n",
       "      <td>-0.104819</td>\n",
       "      <td>-0.050531</td>\n",
       "      <td>-0.220356</td>\n",
       "      <td>-0.062604</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174998</th>\n",
       "      <td>-0.142528</td>\n",
       "      <td>0.036335</td>\n",
       "      <td>0.138094</td>\n",
       "      <td>-0.022376</td>\n",
       "      <td>0.049534</td>\n",
       "      <td>-0.113421</td>\n",
       "      <td>0.134432</td>\n",
       "      <td>-0.096624</td>\n",
       "      <td>-0.050748</td>\n",
       "      <td>-0.087329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.021029</td>\n",
       "      <td>-0.027467</td>\n",
       "      <td>0.106794</td>\n",
       "      <td>-0.042566</td>\n",
       "      <td>-0.206910</td>\n",
       "      <td>-0.094839</td>\n",
       "      <td>-0.097594</td>\n",
       "      <td>-0.207201</td>\n",
       "      <td>-0.130281</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174999</th>\n",
       "      <td>-0.153913</td>\n",
       "      <td>-0.032422</td>\n",
       "      <td>0.167994</td>\n",
       "      <td>-0.028706</td>\n",
       "      <td>0.024664</td>\n",
       "      <td>-0.119468</td>\n",
       "      <td>0.032465</td>\n",
       "      <td>-0.099880</td>\n",
       "      <td>-0.018390</td>\n",
       "      <td>-0.045337</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023295</td>\n",
       "      <td>0.010630</td>\n",
       "      <td>0.086726</td>\n",
       "      <td>-0.047191</td>\n",
       "      <td>-0.195463</td>\n",
       "      <td>-0.047842</td>\n",
       "      <td>-0.068711</td>\n",
       "      <td>-0.184225</td>\n",
       "      <td>-0.067252</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>175000 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0      -0.125472 -0.065518  0.116015 -0.078386 -0.070315 -0.122219  0.017378   \n",
       "1      -0.170451 -0.018026  0.096852 -0.065626  0.072934 -0.109999  0.105688   \n",
       "2      -0.194220 -0.040289  0.091481 -0.017461 -0.019851 -0.126800  0.100452   \n",
       "3      -0.093280 -0.046098  0.146738 -0.053210  0.109236 -0.054069  0.056616   \n",
       "4      -0.150356 -0.012215  0.128281 -0.067569  0.016128 -0.088675  0.093983   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "174995 -0.146697  0.000661  0.108352 -0.014537  0.086563 -0.068104  0.024547   \n",
       "174996 -0.106352  0.031905  0.157137 -0.107703  0.037686 -0.094337  0.061924   \n",
       "174997 -0.141414 -0.001164  0.108775 -0.037603  0.031697 -0.133319  0.009418   \n",
       "174998 -0.142528  0.036335  0.138094 -0.022376  0.049534 -0.113421  0.134432   \n",
       "174999 -0.153913 -0.032422  0.167994 -0.028706  0.024664 -0.119468  0.032465   \n",
       "\n",
       "               7         8         9  ...        21        22        23  \\\n",
       "0      -0.126982  0.037647 -0.064911  ... -0.093300 -0.030247  0.125862   \n",
       "1      -0.139326 -0.024676 -0.038572  ...  0.012400 -0.029055  0.068315   \n",
       "2      -0.153539 -0.021744 -0.032416  ... -0.144041  0.039883  0.146865   \n",
       "3      -0.196522 -0.041214 -0.055622  ... -0.050068  0.019684  0.018286   \n",
       "4      -0.178635  0.011020 -0.070727  ... -0.137437  0.000491  0.095065   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "174995 -0.137124 -0.050225 -0.040411  ... -0.033816  0.013552  0.039731   \n",
       "174996 -0.076955 -0.095596 -0.074425  ... -0.058858 -0.046635  0.119674   \n",
       "174997 -0.163063 -0.022776 -0.055269  ... -0.067018  0.008169  0.094791   \n",
       "174998 -0.096624 -0.050748 -0.087329  ... -0.021029 -0.027467  0.106794   \n",
       "174999 -0.099880 -0.018390 -0.045337  ... -0.023295  0.010630  0.086726   \n",
       "\n",
       "              24        25        26        27        28        29  Class  \n",
       "0      -0.075998 -0.203211 -0.113421 -0.032202 -0.199925 -0.137983      1  \n",
       "1      -0.035170 -0.226974 -0.050105 -0.078412 -0.180631 -0.066747      1  \n",
       "2      -0.086697 -0.240962 -0.144000 -0.091510 -0.206072 -0.155127      1  \n",
       "3       0.022157 -0.161324 -0.015622 -0.043951 -0.208535 -0.073233      1  \n",
       "4      -0.012209 -0.176238 -0.080777 -0.105282 -0.187900 -0.072092      1  \n",
       "...          ...       ...       ...       ...       ...       ...    ...  \n",
       "174995 -0.039308 -0.159591 -0.068564 -0.000229 -0.207346 -0.057379      1  \n",
       "174996 -0.080931 -0.211604 -0.091140 -0.044451 -0.162417 -0.099701      1  \n",
       "174997 -0.040277 -0.154987 -0.104819 -0.050531 -0.220356 -0.062604      1  \n",
       "174998 -0.042566 -0.206910 -0.094839 -0.097594 -0.207201 -0.130281      1  \n",
       "174999 -0.047191 -0.195463 -0.047842 -0.068711 -0.184225 -0.067252      1  \n",
       "\n",
       "[175000 rows x 31 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_fraud_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Time','Amount']] = scaler.fit_transform(df[['Time','Amount']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = df.columns\n",
    "only_fraud_df.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df,only_fraud_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.146374</td>\n",
       "      <td>-0.015519</td>\n",
       "      <td>0.085356</td>\n",
       "      <td>-0.040542</td>\n",
       "      <td>0.066677</td>\n",
       "      <td>-0.081207</td>\n",
       "      <td>0.087645</td>\n",
       "      <td>-0.117641</td>\n",
       "      <td>-0.073402</td>\n",
       "      <td>-0.057336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013769</td>\n",
       "      <td>0.002563</td>\n",
       "      <td>0.101975</td>\n",
       "      <td>-0.060232</td>\n",
       "      <td>-0.191511</td>\n",
       "      <td>-0.090968</td>\n",
       "      <td>-0.066698</td>\n",
       "      <td>-0.206421</td>\n",
       "      <td>-0.086421</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.179971</td>\n",
       "      <td>0.010620</td>\n",
       "      <td>0.158013</td>\n",
       "      <td>-0.114025</td>\n",
       "      <td>0.077457</td>\n",
       "      <td>-0.087706</td>\n",
       "      <td>0.063625</td>\n",
       "      <td>-0.105901</td>\n",
       "      <td>-0.054817</td>\n",
       "      <td>-0.068706</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042756</td>\n",
       "      <td>-0.057189</td>\n",
       "      <td>0.073796</td>\n",
       "      <td>-0.024800</td>\n",
       "      <td>-0.230429</td>\n",
       "      <td>-0.011682</td>\n",
       "      <td>0.010791</td>\n",
       "      <td>-0.156533</td>\n",
       "      <td>-0.053275</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.219835</td>\n",
       "      <td>2.119089</td>\n",
       "      <td>-1.024633</td>\n",
       "      <td>-0.613399</td>\n",
       "      <td>-0.621483</td>\n",
       "      <td>-0.769342</td>\n",
       "      <td>0.432778</td>\n",
       "      <td>-1.214603</td>\n",
       "      <td>0.185715</td>\n",
       "      <td>-0.019076</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.163955</td>\n",
       "      <td>0.156860</td>\n",
       "      <td>0.181343</td>\n",
       "      <td>0.217964</td>\n",
       "      <td>-0.300916</td>\n",
       "      <td>0.709351</td>\n",
       "      <td>-0.015154</td>\n",
       "      <td>-0.055179</td>\n",
       "      <td>-0.325371</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.099612</td>\n",
       "      <td>0.048388</td>\n",
       "      <td>0.174961</td>\n",
       "      <td>-0.056897</td>\n",
       "      <td>0.157342</td>\n",
       "      <td>-0.086699</td>\n",
       "      <td>-0.008830</td>\n",
       "      <td>-0.129328</td>\n",
       "      <td>-0.082722</td>\n",
       "      <td>-0.063709</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048781</td>\n",
       "      <td>0.036051</td>\n",
       "      <td>0.025908</td>\n",
       "      <td>-0.005768</td>\n",
       "      <td>-0.131239</td>\n",
       "      <td>-0.022695</td>\n",
       "      <td>-0.069082</td>\n",
       "      <td>-0.209291</td>\n",
       "      <td>-0.020680</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.327164</td>\n",
       "      <td>1.181842</td>\n",
       "      <td>-0.220696</td>\n",
       "      <td>0.650137</td>\n",
       "      <td>-0.166884</td>\n",
       "      <td>-0.770594</td>\n",
       "      <td>-0.379380</td>\n",
       "      <td>-0.480922</td>\n",
       "      <td>0.187963</td>\n",
       "      <td>0.333115</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.057635</td>\n",
       "      <td>-0.225252</td>\n",
       "      <td>0.156492</td>\n",
       "      <td>0.247896</td>\n",
       "      <td>-0.060545</td>\n",
       "      <td>0.908772</td>\n",
       "      <td>-0.067108</td>\n",
       "      <td>-0.005120</td>\n",
       "      <td>-0.344421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458721</th>\n",
       "      <td>1.360712</td>\n",
       "      <td>0.149260</td>\n",
       "      <td>0.988698</td>\n",
       "      <td>-0.605170</td>\n",
       "      <td>-0.788264</td>\n",
       "      <td>1.238822</td>\n",
       "      <td>-0.203064</td>\n",
       "      <td>0.861209</td>\n",
       "      <td>0.067349</td>\n",
       "      <td>-0.279204</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.326673</td>\n",
       "      <td>-0.816542</td>\n",
       "      <td>0.008957</td>\n",
       "      <td>-0.010456</td>\n",
       "      <td>-0.377837</td>\n",
       "      <td>0.128193</td>\n",
       "      <td>0.219217</td>\n",
       "      <td>0.068558</td>\n",
       "      <td>-0.349773</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458722</th>\n",
       "      <td>-0.144409</td>\n",
       "      <td>-0.056656</td>\n",
       "      <td>0.148645</td>\n",
       "      <td>-0.104169</td>\n",
       "      <td>0.046285</td>\n",
       "      <td>-0.127849</td>\n",
       "      <td>-0.003682</td>\n",
       "      <td>-0.101806</td>\n",
       "      <td>-0.062445</td>\n",
       "      <td>-0.060322</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030523</td>\n",
       "      <td>-0.052311</td>\n",
       "      <td>0.109520</td>\n",
       "      <td>-0.045674</td>\n",
       "      <td>-0.214116</td>\n",
       "      <td>-0.088019</td>\n",
       "      <td>-0.053956</td>\n",
       "      <td>-0.185627</td>\n",
       "      <td>-0.087897</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458723</th>\n",
       "      <td>-0.312295</td>\n",
       "      <td>1.259310</td>\n",
       "      <td>-0.049484</td>\n",
       "      <td>-0.721776</td>\n",
       "      <td>0.071903</td>\n",
       "      <td>1.864771</td>\n",
       "      <td>3.635628</td>\n",
       "      <td>-0.821682</td>\n",
       "      <td>0.929256</td>\n",
       "      <td>0.128685</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070189</td>\n",
       "      <td>0.104264</td>\n",
       "      <td>-0.107993</td>\n",
       "      <td>1.004823</td>\n",
       "      <td>0.674600</td>\n",
       "      <td>-0.275199</td>\n",
       "      <td>0.045933</td>\n",
       "      <td>0.025074</td>\n",
       "      <td>-0.335356</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458724</th>\n",
       "      <td>-0.132160</td>\n",
       "      <td>1.982903</td>\n",
       "      <td>-0.134427</td>\n",
       "      <td>-1.161183</td>\n",
       "      <td>0.472515</td>\n",
       "      <td>-0.043755</td>\n",
       "      <td>-1.080473</td>\n",
       "      <td>0.306983</td>\n",
       "      <td>-0.373167</td>\n",
       "      <td>0.454594</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.212230</td>\n",
       "      <td>-0.472417</td>\n",
       "      <td>0.248672</td>\n",
       "      <td>0.025584</td>\n",
       "      <td>-0.195097</td>\n",
       "      <td>0.272711</td>\n",
       "      <td>-0.071660</td>\n",
       "      <td>-0.055715</td>\n",
       "      <td>-0.195658</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458725</th>\n",
       "      <td>-0.384429</td>\n",
       "      <td>-0.465150</td>\n",
       "      <td>0.818433</td>\n",
       "      <td>1.326391</td>\n",
       "      <td>-1.345201</td>\n",
       "      <td>0.584115</td>\n",
       "      <td>-0.683284</td>\n",
       "      <td>0.856022</td>\n",
       "      <td>-0.178265</td>\n",
       "      <td>-0.537311</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.235506</td>\n",
       "      <td>-0.719605</td>\n",
       "      <td>-0.290143</td>\n",
       "      <td>-0.387924</td>\n",
       "      <td>0.413861</td>\n",
       "      <td>0.813386</td>\n",
       "      <td>-0.130873</td>\n",
       "      <td>-0.047641</td>\n",
       "      <td>-0.353327</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>458726 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0      -0.146374 -0.015519  0.085356 -0.040542  0.066677 -0.081207  0.087645   \n",
       "1      -0.179971  0.010620  0.158013 -0.114025  0.077457 -0.087706  0.063625   \n",
       "2       1.219835  2.119089 -1.024633 -0.613399 -0.621483 -0.769342  0.432778   \n",
       "3      -0.099612  0.048388  0.174961 -0.056897  0.157342 -0.086699 -0.008830   \n",
       "4      -0.327164  1.181842 -0.220696  0.650137 -0.166884 -0.770594 -0.379380   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "458721  1.360712  0.149260  0.988698 -0.605170 -0.788264  1.238822 -0.203064   \n",
       "458722 -0.144409 -0.056656  0.148645 -0.104169  0.046285 -0.127849 -0.003682   \n",
       "458723 -0.312295  1.259310 -0.049484 -0.721776  0.071903  1.864771  3.635628   \n",
       "458724 -0.132160  1.982903 -0.134427 -1.161183  0.472515 -0.043755 -1.080473   \n",
       "458725 -0.384429 -0.465150  0.818433  1.326391 -1.345201  0.584115 -0.683284   \n",
       "\n",
       "              V7        V8        V9  ...       V21       V22       V23  \\\n",
       "0      -0.117641 -0.073402 -0.057336  ...  0.013769  0.002563  0.101975   \n",
       "1      -0.105901 -0.054817 -0.068706  ... -0.042756 -0.057189  0.073796   \n",
       "2      -1.214603  0.185715 -0.019076  ... -0.163955  0.156860  0.181343   \n",
       "3      -0.129328 -0.082722 -0.063709  ... -0.048781  0.036051  0.025908   \n",
       "4      -0.480922  0.187963  0.333115  ... -0.057635 -0.225252  0.156492   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "458721  0.861209  0.067349 -0.279204  ... -0.326673 -0.816542  0.008957   \n",
       "458722 -0.101806 -0.062445 -0.060322  ...  0.030523 -0.052311  0.109520   \n",
       "458723 -0.821682  0.929256  0.128685  ...  0.070189  0.104264 -0.107993   \n",
       "458724  0.306983 -0.373167  0.454594  ... -0.212230 -0.472417  0.248672   \n",
       "458725  0.856022 -0.178265 -0.537311  ... -0.235506 -0.719605 -0.290143   \n",
       "\n",
       "             V24       V25       V26       V27       V28    Amount  Class  \n",
       "0      -0.060232 -0.191511 -0.090968 -0.066698 -0.206421 -0.086421      1  \n",
       "1      -0.024800 -0.230429 -0.011682  0.010791 -0.156533 -0.053275      1  \n",
       "2       0.217964 -0.300916  0.709351 -0.015154 -0.055179 -0.325371      0  \n",
       "3      -0.005768 -0.131239 -0.022695 -0.069082 -0.209291 -0.020680      1  \n",
       "4       0.247896 -0.060545  0.908772 -0.067108 -0.005120 -0.344421      0  \n",
       "...          ...       ...       ...       ...       ...       ...    ...  \n",
       "458721 -0.010456 -0.377837  0.128193  0.219217  0.068558 -0.349773      0  \n",
       "458722 -0.045674 -0.214116 -0.088019 -0.053956 -0.185627 -0.087897      1  \n",
       "458723  1.004823  0.674600 -0.275199  0.045933  0.025074 -0.335356      0  \n",
       "458724  0.025584 -0.195097  0.272711 -0.071660 -0.055715 -0.195658      0  \n",
       "458725 -0.387924  0.413861  0.813386 -0.130873 -0.047641 -0.353327      0  \n",
       "\n",
       "[458726 rows x 31 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exported Generated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('D:/Uni Docs/DSC4996/Dynamic_fraud_detection_system/Data/generated_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
